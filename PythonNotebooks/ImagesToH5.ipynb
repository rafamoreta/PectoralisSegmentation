{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pectoralis Segmentation: Convert to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "# %matplotlib inline\n",
    "import h5py\n",
    "import progressbar\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(f):\n",
    "    return [key for key in f.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(h5_path):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        keys = [key for key in f.keys()]\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_newH5(output_path):\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "    \n",
    "def create_dset(h5_path, dataKey, dataShape, dim_maxsize, dtype, data):\n",
    "    with h5py.File(h5_path, 'r+') as f:\n",
    "        maxshape = list(dataShape[:])\n",
    "        maxshape[dim_maxsize] = None\n",
    "        maxshape = tuple(maxshape)\n",
    "        \n",
    "        chunks = list(dataShape[:])\n",
    "        chunks[dim_maxsize] = 1\n",
    "        chunks = tuple(chunks)\n",
    "        \n",
    "        data_orig = f.create_dataset(dataKey, dataShape, dtype=dtype, maxshape=maxshape, data=data, chunks=chunks)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_dset_no_data(h5_path, dataKey, dataShape, dim_maxsize, dtype):\n",
    "    with h5py.File(h5_path, 'r+') as f:\n",
    "        maxshape = list(dataShape[:])\n",
    "        maxshape[dim_maxsize] = None\n",
    "        maxshape = tuple(maxshape)\n",
    "        \n",
    "        chunks = list(dataShape[:])\n",
    "        chunks[dim_maxsize] = 1\n",
    "        chunks = tuple(chunks)\n",
    "        \n",
    "        data_orig = f.create_dataset(dataKey, dataShape, dtype=dtype, maxshape=maxshape, chunks=chunks)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def add_to_dset(h5_path, dataKey, idim_to_add, new_data):\n",
    "    with h5py.File(h5_path, 'r+') as f:\n",
    "        ## Comprobar que el dset existe con la funcion keys\n",
    "        dset = f[dataKey]\n",
    "        dset_shape = list(dset.shape)\n",
    "        new_dataShape = list(new_data.shape)\n",
    "        \n",
    "#         print('previos shape: ', dset_shape)\n",
    "#         print('previos shape: ', new_dataShape)\n",
    "        a = dset_shape[idim_to_add]\n",
    "        b = new_dataShape[idim_to_add]\n",
    "        dset_shape[idim_to_add] = a + b\n",
    "    \n",
    "#         print('new final shape: ', dset_shape)\n",
    "        dset.resize(tuple(dset_shape))\n",
    "        \n",
    "        if idim_to_add == 0:\n",
    "            dset[a:] = new_data\n",
    "    \n",
    "    return None\n",
    "        \n",
    "def modify_dset(h5_path, dataKey, indexes, data):\n",
    "    with h5py.File(h5_path, 'r+') as f:\n",
    "        ## Comprobar que el dset existe con la funcion keys\n",
    "        dset = f[dataKey]\n",
    "        dset[indexes] = data\n",
    "    return None\n",
    "\n",
    "def delete_dset(h5_path, dataKey):\n",
    "    with h5py.File(h5_path, 'r+') as f:\n",
    "        del f[dataKey]\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_shape_dset(h5_path, dataKey):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        shape = f[dataKey].shape\n",
    "    return shape\n",
    "\n",
    "\n",
    "def get_dset_idx(h5_path, dataKey, idx):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        data = f[dataKey][idx]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Info from h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "folder_daneses = '/data/DNNData/la660/Data/DataSet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = get_keys(h5_path)\n",
    "\n",
    "for key in keys:\n",
    "    print(key, ': \\t\\t', get_shape_dset(h5_path, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding H5PY DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates new H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create H5\n",
    "output_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "# create_newH5(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Images data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/rmoreta/Documents/BWH Estancia 2016/ProjectData_clean/'\n",
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "widgets = [\"Saving to h5: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=7*2, widgets=widgets).start()\n",
    "\n",
    "dataKey = 'images_raw'\n",
    "h5_keys = get_keys(h5_path)\n",
    "if dataKey in h5_keys:\n",
    "    print('Key already existed. Now is deleted.', dataKey)\n",
    "    delete_dset(h5_path, dataKey)\n",
    "\n",
    "num_files = range(7)\n",
    "for i in num_files:\n",
    "    # Reading Slices\n",
    "    file_name_CT = 'Cont_' + str(i+1) + '_clean.nrrd'\n",
    "    image_nrrd = sitk.ReadImage(folder + file_name_CT)\n",
    "    CTSlices = sitk.GetArrayFromImage(image_nrrd)\n",
    "    dataShape = CTSlices.shape\n",
    "    print(file_name_CT, 'shape: ', dataShape)\n",
    "    pbar.update((i*2))\n",
    "    \n",
    "    if i == num_files[0]:\n",
    "        ## Create dataset in H5\n",
    "        create_dset(h5_path, dataKey, dataShape, 0, 'f', CTSlices)\n",
    "        \n",
    "    else:\n",
    "        ## Add data to a extinting dataset\n",
    "        idim_to_add = 0\n",
    "        add_to_dset(h5_path, dataKey, idim_to_add, CTSlices)\n",
    "        \n",
    "    pbar.update(i*2+1)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/rmoreta/Documents/BWH Estancia 2016/ProjectData_clean/'\n",
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "dataKey = 'labels_raw'\n",
    "h5_keys = get_keys(h5_path)\n",
    "if dataKey in h5_keys:\n",
    "    print('Key already existed. Now is deleted.', dataKey)\n",
    "    delete_dset(h5_path, dataKey)\n",
    "    \n",
    "widgets = [\"Saving to h5: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=7*2, widgets=widgets).start()\n",
    "\n",
    "num_files = range(7)\n",
    "for i in num_files:\n",
    "    # Reading Slices\n",
    "    file_name_label = 'Cont_lm_' + str(i+1) + '_clean.nrrd'\n",
    "    image_nrrd = sitk.ReadImage(folder + file_name_label)\n",
    "    labels = sitk.GetArrayFromImage(image_nrrd)\n",
    "    dataShape = labels.shape\n",
    "    print(file_name_label, 'shape: ', dataShape)\n",
    "    pbar.update((i*2))\n",
    "    \n",
    "    if i == num_files[0]:\n",
    "        ## Create dataset in H5\n",
    "        create_dset(h5_path, dataKey, dataShape, 0, 'f', labels)\n",
    "        \n",
    "    else:\n",
    "        ## Add data to a extinting dataset\n",
    "        idim_to_add = 0\n",
    "        add_to_dset(h5_path, dataKey, idim_to_add, labels)\n",
    "    pbar.update(i*2+1)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_images(data):\n",
    "    norm_data = data.astype(np.float32)\n",
    "    norm_data[norm_data < -1024] = -1024\n",
    "    norm_data[norm_data > 1500] = 1500\n",
    "    norm_data = (norm_data + 1024) / (1500 + 1024) - 0.5\n",
    "\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/rmoreta/Documents/BWH Estancia 2016/ProjectData_clean/'\n",
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "dataKey = 'images_norm'\n",
    "h5_keys = get_keys(h5_path)\n",
    "if dataKey in h5_keys:\n",
    "    print('Key already existed. Now is deleted.')\n",
    "    delete_dset(h5_path, dataKey)\n",
    "\n",
    "dset_size = get_shape_dset(h5_path, 'images_raw')\n",
    "im_size = (0, dset_size[1], dset_size[2])\n",
    "create_dset_no_data(h5_path, dataKey, im_size, 0, 'f')\n",
    "len_dset = dset_size[0]\n",
    "    \n",
    "widgets = [\"Saving to h5: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=len_dset, widgets=widgets).start()\n",
    "\n",
    "buff_size = 1000\n",
    "for i in np.arange(0, len_dset, buff_size):\n",
    "    if i+buff_size > len_dset:\n",
    "        idx = range(i, len_dset)\n",
    "    else:\n",
    "        idx = range(i, i+buff_size)\n",
    "        \n",
    "    data = get_dset_idx(h5_path, 'images_raw', idx)\n",
    "    data_norm = norm_images(data)\n",
    "    add_to_dset(h5_path, dataKey, 0, data_norm)\n",
    "        \n",
    "    pbar.update(i)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels n classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_labels(labels, num_classes):\n",
    "    ERROR = True\n",
    "    if num_classes == 2:\n",
    "        labels[(labels!=13335)&(labels!=13336)&(labels!=13591)&(labels!=13592)&(labels!=17943)&(labels!=17944)] = 0\n",
    "        labels[(labels==13335)] = 1 # Left Minor Pectoral\n",
    "        labels[(labels==13336)] = 1 # Right Minor Pectoral\n",
    "        labels[(labels==13591)] = 1 # Left Major Pectoral\n",
    "        labels[(labels==13592)] = 1 # RIght Major Pectoral\n",
    "        labels[(labels==17943)] = 0 # Left Fat \n",
    "        labels[(labels==17944)] = 0 # Right Fat \n",
    "        \n",
    "    elif num_classes == 5:\n",
    "        labels[(labels!=13335)&(labels!=13336)&(labels!=13591)&(labels!=13592)&(labels!=17943)&(labels!=17944)] = 0\n",
    "        labels[(labels==13335)] = 1 # Left Minor Pectoral\n",
    "        labels[(labels==13336)] = 2 # Right Minor Pectoral\n",
    "        labels[(labels==13591)] = 3 # Left Major Pectoral\n",
    "        labels[(labels==13592)] = 4 # RIght Major Pectoral\n",
    "        labels[(labels==17943)] = 0 # Left Fat \n",
    "        labels[(labels==17944)] = 0 # Right Fat\n",
    "        \n",
    "    elif num_classes == 3:\n",
    "        labels[(labels!=13335)&(labels!=13336)&(labels!=13591)&(labels!=13592)&(labels!=17943)&(labels!=17944)] = 0\n",
    "        labels[(labels==13335)] = 0 # Left Minor Pectoral\n",
    "        labels[(labels==13336)] = 0 # Right Minor Pectoral\n",
    "        labels[(labels==13591)] = 0 # Left Major Pectoral\n",
    "        labels[(labels==13592)] = 0 # RIght Major Pectoral\n",
    "        labels[(labels==17943)] = 1 # Left Fat \n",
    "        labels[(labels==17944)] = 2 # Right Fat \n",
    "        \n",
    "    else:\n",
    "        print('Introduce the follwoing number of classes: 2,3 or 5')\n",
    "        ERROR = False\n",
    "\n",
    "    if ERROR: labels_cat = (np.arange(num_classes) == labels[:,:,:,None])\n",
    "        \n",
    "    return labels_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/rmoreta/Documents/BWH Estancia 2016/ProjectData_clean/'\n",
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "dataKey_list = ['labels_nc2_pecs', 'labels_nc5_pecs', 'labels_nc3_fat']\n",
    "num_classes_list = [2,5,3]\n",
    "# dataKey_list = ['labels_nc2_pecs_borrar']\n",
    "# num_classes_list = [5]\n",
    "\n",
    "for ii in range(len(dataKey_list)):\n",
    "    h5_keys = get_keys(h5_path)\n",
    "    dataKey = dataKey_list[ii]\n",
    "    num_classes = num_classes_list[ii]\n",
    "    \n",
    "    if dataKey in h5_keys:\n",
    "        print('Key already existed. Now is deleted.')\n",
    "        delete_dset(h5_path, dataKey)\n",
    "\n",
    "    dset_size = get_shape_dset(h5_path, 'labels_raw')\n",
    "    im_size = (0, dset_size[1], dset_size[2], num_classes)\n",
    "    create_dset_no_data(h5_path, dataKey, im_size, 0, 'bool')\n",
    "    len_dset = dset_size[0]\n",
    "\n",
    "    widgets = [\"Saving to h5: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval=len_dset, widgets=widgets).start()\n",
    "\n",
    "    buff_size = 1000\n",
    "    for i in np.arange(0, len_dset, buff_size):\n",
    "        if i+buff_size > len_dset:\n",
    "            idx = range(i, len_dset)\n",
    "        else:\n",
    "            idx = range(i, i+buff_size)\n",
    "            \n",
    "        data = get_dset_idx(h5_path, 'labels_raw', idx)\n",
    "        data_labels = separate_labels(data, num_classes)\n",
    "        add_to_dset(h5_path, dataKey, 0, data_labels)\n",
    "\n",
    "        pbar.update(i)\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Scan ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = '/Volumes/GoogleDrive/Team Drives/GR.INV. BIIG-IGT/Projects/Pectoralis Segmentation/Data/pectoralis_copd_slices_clean_test_train.csv'\n",
    "path_csv = '/Volumes/GoogleDrive/Team Drives/GR.INV. BIIG-IGT/Projects/Pectoralis Segmentation/Data/pectoralis_copd_slices_clean.csv'\n",
    "\n",
    "csv_data = pd.read_csv(path_csv,',')\n",
    "print(list(csv_data.keys()))\n",
    "\n",
    "indexes = csv_data['bad_segmentatio'] == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scan ID*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "dataKey = 'scan_id'\n",
    "h5_keys = get_keys(h5_path)\n",
    "if dataKey in h5_keys:\n",
    "    print('Key already existed. Now is deleted.')\n",
    "    delete_dset(h5_path, dataKey)\n",
    "\n",
    "data = csv_data['scan_id'][indexes].values\n",
    "dataShape = data.shape    \n",
    "\n",
    "create_dset(h5_path, dataKey, dataShape, 0, 'f', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*code* ((NEEDS TO BE FIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "dataKey = 'code'\n",
    "h5_keys = get_keys(h5_path)\n",
    "if dataKey in h5_keys:\n",
    "    print('Key already existed. Now is deleted.')\n",
    "    delete_dset(h5_path, dataKey)\n",
    "\n",
    "data = np.string_(csv_data[dataKey][indexes].values)\n",
    "# dataShape = (10006,)\n",
    "\n",
    "create_dset(h5_path, dataKey, (10006,), 0, 'S10', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*'slice', 'container_id', 'container_slice'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'\n",
    "\n",
    "dataKey = 'slice_&_container'\n",
    "h5_keys = get_keys(h5_path)\n",
    "if dataKey in h5_keys:\n",
    "    print('Key already existed. Now is deleted.')\n",
    "    delete_dset(h5_path, dataKey)\n",
    "\n",
    "csv_keys = ['slice', 'container_id', 'container_slice']\n",
    "data = []\n",
    "for csv_key in csv_keys:\n",
    "    aux = csv_data[csv_key][indexes].values\n",
    "    data.append(aux)\n",
    "data = np.swapaxes(np.array(data),0,1)\n",
    "dataShape = data.shape    \n",
    "\n",
    "create_dset(h5_path, dataKey, dataShape, 0, 'f', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if it has been saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/rmoreta/Documents/BWH Estancia 2016/ProjectData_clean/'\n",
    "h5_path = '/data/DNNData/rm196/PectoralisSegmentation/Data/myfinaltestfile.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = get_keys(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = [6] # 7 nrrd files in total\n",
    "\n",
    "CTSlices = []\n",
    "labels = []\n",
    "# Reading CT\n",
    "\n",
    "for i in num_files:\n",
    "    if i == num_files[0]:\n",
    "        # CTSlices\n",
    "        file_name_CT = 'Cont_' + str(i+1) + '_clean.nrrd'#'_clean.nrrd'\n",
    "        image_nrrd = sitk.ReadImage(folder + file_name_CT)\n",
    "        CTSlices = sitk.GetArrayFromImage(image_nrrd)\n",
    "        # Labels\n",
    "        file_name_label = 'Cont_lm_' + str(i+1) + '_clean.nrrd'\n",
    "        image_nrrd = sitk.ReadImage(folder + file_name_label)\n",
    "        labels = sitk.GetArrayFromImage(image_nrrd)\n",
    "    else:\n",
    "        # CTSlices\n",
    "        file_name_CT = 'Cont_' + str(i+1) + '_clean.nrrd'\n",
    "        image_nrrd = sitk.ReadImage(folder + file_name_CT)\n",
    "        CTSlices_aux = sitk.GetArrayFromImage(image_nrrd)\n",
    "        CTSlices = np.concatenate((CTSlices, CTSlices_aux), axis = 0)\n",
    "        # Labels\n",
    "        file_name_label = 'Cont_lm_' + str(i+1) + '_clean.nrrd'\n",
    "        image_nrrd = sitk.ReadImage(folder + file_name_label)\n",
    "        labels_aux = sitk.GetArrayFromImage(image_nrrd)\n",
    "        labels = np.concatenate((labels, labels_aux), axis = 0)\n",
    "        \n",
    "        \n",
    "print('CTSlices Shape: ')\n",
    "print(CTSlices.shape)\n",
    "\n",
    "print('Labels Shape: ')\n",
    "print(labels.shape)\n",
    "\n",
    "num_imgs_list = [1455, 1449, 1435, 1451, 1435, 1444, 1337]\n",
    "num_imgs_cont = CTSlices.shape[0]\n",
    "\n",
    "fr = int(np.sum(num_imgs_list[0:i]))\n",
    "to = fr + num_imgs_cont "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_one_channel(images):\n",
    "    num_imgs = len(images)\n",
    "    f, a = plt.subplots(1, num_imgs, figsize=(20, 20))\n",
    "    for i in range(num_imgs):\n",
    "        a[i].imshow(images[i],cmap='Greys_r')\n",
    "    f.show()\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(num_imgs_cont)\n",
    "i_h5 = (i + int(np.sum(num_imgs_list[0:num_files[0]])))\n",
    "\n",
    "raw_img = CTSlices[i]\n",
    "raw_label = labels[i]\n",
    "\n",
    "raw_img_h5 = get_dset_idx(h5_path, 'images_raw', i_h5)\n",
    "raw_labels_h5 = get_dset_idx(h5_path, 'labels_raw', i_h5)\n",
    "\n",
    "plot_images_one_channel([raw_img, raw_img_h5])\n",
    "plot_images_one_channel([raw_label, raw_labels_h5])\n",
    "\n",
    "if np.array_equal(raw_img, raw_img_h5):\n",
    "    print('Raw images are the same')\n",
    "    \n",
    "if np.array_equal(raw_label, raw_labels_h5):\n",
    "    print('Raw labels are the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = CTSlices.astype(np.float32)\n",
    "numpy_array[numpy_array < -1024] = -1024\n",
    "numpy_array[numpy_array > 1500] = 1500\n",
    "numpy_array = (numpy_array + 1024) / (1500 + 1024) - 0.5\n",
    "\n",
    "CTSlices_new = numpy_array\n",
    "print(CTSlices_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(num_imgs_cont)\n",
    "i_h5 = (i + int(np.sum(num_imgs_list[0:num_files[0]])))\n",
    "print('image: ', i, i_h5)\n",
    "\n",
    "norm_img = CTSlices_new[i]\n",
    "\n",
    "norm_img_h5 = get_dset_idx(h5_path, 'images_norm', i_h5)\n",
    "\n",
    "plot_images_one_channel([norm_img, norm_img_h5])\n",
    "\n",
    "if np.array_equal(norm_img, norm_img_h5):\n",
    "    print('Raw images are the same')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels number classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_labels(labels, num_classes):\n",
    "    ERROR = False\n",
    "    if num_classes == 2:\n",
    "        labels[(labels!=13335)&(labels!=13336)&(labels!=13591)&(labels!=13592)&(labels!=17943)&(labels!=17944)] = 0\n",
    "        labels[(labels==13335)] = 1 # Left Minor Pectoral\n",
    "        labels[(labels==13336)] = 1 # Right Minor Pectoral\n",
    "        labels[(labels==13591)] = 1 # Left Major Pectoral\n",
    "        labels[(labels==13592)] = 1 # RIght Major Pectoral\n",
    "        labels[(labels==17943)] = 0 # Left Fat \n",
    "        labels[(labels==17944)] = 0 # Right Fat \n",
    "        \n",
    "    elif num_classes == 5:\n",
    "        labels[(labels!=13335)&(labels!=13336)&(labels!=13591)&(labels!=13592)&(labels!=17943)&(labels!=17944)] = 0\n",
    "        labels[(labels==13335)] = 1 # Left Minor Pectoral\n",
    "        labels[(labels==13336)] = 2 # Right Minor Pectoral\n",
    "        labels[(labels==13591)] = 3 # Left Major Pectoral\n",
    "        labels[(labels==13592)] = 4 # RIght Major Pectoral\n",
    "        labels[(labels==17943)] = 0 # Left Fat \n",
    "        labels[(labels==17944)] = 0 # Right Fat\n",
    "        \n",
    "    elif num_classes == 3:\n",
    "        labels[(labels!=13335)&(labels!=13336)&(labels!=13591)&(labels!=13592)&(labels!=17943)&(labels!=17944)] = 0\n",
    "        labels[(labels==13335)] = 0 # Left Minor Pectoral\n",
    "        labels[(labels==13336)] = 0 # Right Minor Pectoral\n",
    "        labels[(labels==13591)] = 0 # Left Major Pectoral\n",
    "        labels[(labels==13592)] = 0 # RIght Major Pectoral\n",
    "        labels[(labels==17943)] = 1 # Left Fat \n",
    "        labels[(labels==17944)] = 2 # Right Fat \n",
    "        \n",
    "    else:\n",
    "        print('Introduce the follwoing number of classes: 2,3 or 5')\n",
    "        ERROR = True\n",
    "\n",
    "    if not(ERROR): labels_cat = (np.arange(num_classes) == labels[:,:,:,None])\n",
    "        \n",
    "    return labels_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lables_nc(labels_nc, num_classes, var):\n",
    "    if var == 0: labels_nc = labels_nc[0]\n",
    "    \n",
    "    f, a = plt.subplots(1, num_classes, figsize=(20, 20))\n",
    "    for i in range(num_classes):\n",
    "        a[i].imshow(labels_nc[:,:,i],cmap='Greys_r')\n",
    "    f.show()\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(num_imgs_cont)\n",
    "i_h5 = (i + int(np.sum(num_imgs_list[0:num_files[0]])))\n",
    "print('image: ', i, i_h5)\n",
    "\n",
    "dataKey_list = ['labels_nc2_pecs', 'labels_nc5_pecs', 'labels_nc3_fat']\n",
    "num_classes_list = [2,5,3]\n",
    "\n",
    "raw_img = CTSlices[i]\n",
    "plot_images_one_channel([raw_img, raw_img])\n",
    "\n",
    "for dataKey, nc in zip(dataKey_list, num_classes_list):\n",
    "    \n",
    "    image_nrrd = sitk.ReadImage(folder + file_name_label)\n",
    "    labels = sitk.GetArrayFromImage(image_nrrd)\n",
    "    \n",
    "    labels_nc = separate_labels(labels[i:i+1], nc)\n",
    "\n",
    "    labels_nc_h5 = get_dset_idx(h5_path, dataKey, i_h5)\n",
    "\n",
    "    print(labels_nc.shape)\n",
    "    if np.array_equal(labels_nc[0], labels_nc_h5):\n",
    "        print('Raw images are the same')\n",
    "    \n",
    "    plot_lables_nc(labels_nc, nc, 0)\n",
    "    plot_lables_nc(labels_nc_h5, nc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
